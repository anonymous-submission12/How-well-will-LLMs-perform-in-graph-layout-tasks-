{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2ff773-ea3c-44f1-af96-5f2088831324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import os\n",
    "import json as js\n",
    "import glob\n",
    "import re\n",
    "from itertools import product\n",
    "from itertools import cycle\n",
    "import time\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aade1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomError(RuntimeError):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292c5586-f485-48e0-961f-3a7f8523de2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = Path(\"logs/async_requests.log\")\n",
    "log_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file, mode=\"a\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa6cf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 3 #DELAY BY SECONDS\n",
    "\n",
    "RETRY_FILE = \"retry_gpt.txt\"\n",
    "\n",
    "SEMAPHORE = asyncio.Semaphore(50) # Decrease if toooo big\n",
    "\n",
    "endpoints = \"YOUR END POINTS\"\n",
    "api_keys = \"YOUR API KEYS\"\n",
    "\n",
    "endpoint_cycle = cycle(endpoints)\n",
    "api_key_cycle = cycle(api_keys)\n",
    "\n",
    "def get_next_endpoint_and_key():\n",
    "    raise NotImplementedError()\n",
    "    #return next(endpoint_cycle), next(api_key_cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa00447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ids(directory):\n",
    "    integers = set()\n",
    "    pattern = re.compile(r'^(\\d+)_')\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            integers.add(int(match.group(1)))\n",
    "    \n",
    "    return sorted(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8056fa75-c08a-4f57-b1a1-3acf526d6328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_graph_data_by_pk(fmt:str, p_alias:str, size:str, index:int) -> str:\n",
    "    \"\"\"Find the path of graph data file for (FMT, P_ALIAS, SIZE, INDEX)\"\"\"\n",
    "    if size not in [\"exlarge\", \"small\"]:\n",
    "        raise CustomError(f\"Invalid size {size}.\")\n",
    "    if fmt not in [\"edge_list\"]:\n",
    "        raise CustomError(f\"Invalid fmt {fmt}.\")\n",
    "\n",
    "    term = \"general\" if p_alias == \"SBM\" else \"special\"\n",
    "\n",
    "    str_pattern = f\"{index}_{p_alias}_*.txt\"\n",
    "    files = glob.glob(f\"data/{size}_{term}_graphs/{fmt}/{p_alias}/{str_pattern}\")\n",
    "\n",
    "    for file in files:\n",
    "        match = re.match(rf\"data/{size}_{term}_graphs/{fmt}/{p_alias}/(\\d+)_.*\\.txt$\", file)\n",
    "        if match and int(match.group(1)) == index:\n",
    "            return file\n",
    "\n",
    "    \n",
    "    raise CustomError(f\"Filename not found for {fmt} {p_alias} {size} {index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9522174c-1f97-4147-8e77-5d82f60792d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_graph_from_edge_list(filename:str):\n",
    "    edges = []\n",
    "    nodes = set()\n",
    "\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            node1, node2 = map(int, line.split())\n",
    "            \n",
    "            edges.append((node1, node2))\n",
    "            \n",
    "            nodes.add(node1)\n",
    "            nodes.add(node2)\n",
    "    \n",
    "    return sorted(list(nodes)), sorted(list(edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dca9dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_alias(pattern: str) -> str:\n",
    "    if pattern not in [\"Cycle\",\"Star\", \"Path\", \"Grid\", \"clustered graph\"]:\n",
    "        raise CustomError(f\"Invalid pattern {pattern}\")\n",
    "    if pattern == \"clustered graph\":\n",
    "        p_alias = \"SBM\"\n",
    "    else:\n",
    "        p_alias = pattern\n",
    "    return p_alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293c2d3d-5cb9-461e-9458-a43e27faf56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_graph_to_str_el(f):\n",
    "    n, e = read_graph_from_edge_list(f)\n",
    "    return n, str(e)\n",
    "\n",
    "async def perform_test(session, test_pks:list, p_alias:str, ids:list, debug=False, virtual=True, console=True):\n",
    "    tasks = []\n",
    "    valid_requirements = list(range(2, 5)) if p_alias == \"SBM\" else list(range(2, 4))\n",
    "    \n",
    "    debug_count = 0\n",
    "    fmt, model, size, requirement, allow_existing = test_pks\n",
    "\n",
    "    if requirement not in valid_requirements:\n",
    "        raise CustomError(f\"Invalid requirement {requirement} for {p_alias}.\")\n",
    "\n",
    "\n",
    "    for i in ids[size]:\n",
    "        f = find_graph_data_by_pk(fmt, p_alias, size, i)\n",
    "\n",
    "        debug_count += 1\n",
    "        if debug == True and debug_count > 1:\n",
    "            break\n",
    "\n",
    "        nodes, graph_data = parse_graph_to_str_el(f)\n",
    "            \n",
    "        content = generate_prompt_q2(graph_data, fmt, requirement, allow_existing, console=virtual)\n",
    "        if not virtual: \n",
    "            tasks.append(create_completion(session, content, p_alias, graph_data, fmt, model, size, i, requirement, allow_existing, debug=debug))\n",
    "        else:\n",
    "            v_c = v_completion(content, model)\n",
    "            tasks.append(v_c)\n",
    "            if console:\n",
    "                print(v_c)\n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fd280a-57b4-43c5-a4fa-5255c96a8a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def do_task(patterns: list, d_ids:list=None, d_fmt=None, d_model=None, d_size=None, d_requirements:list=None, d_allow_existing:bool=None, debug=False, virtual=True, console=True):\n",
    "    \"\"\"Perform tests on task data determined by PATTERN.\\n\n",
    "        Parameters starting with \"d_\" are optional, use them if dimensions of the task are specified.\\n\n",
    "        Set DEBUG to True to test with a few runs.\\n\n",
    "        Set VIRTUAL to True to inspect the prompts generated without sending any actual requests.\n",
    "        Set CONSOLE to False to disable print statements.\n",
    "        \"\"\"\n",
    "    fmts = [\"edge_list\"]\n",
    "    models = [\"gpt-4o-2024-11-20\", \"deepseek-v3\", \"gemini-2.0-flash-001\"]\n",
    "    sizes = [\"exlarge\",\"small\"]\n",
    "    requirements = list(range(2, 4))\n",
    "    allow_existings = [True, False]\n",
    "\n",
    "    if type(patterns) != list:\n",
    "        raise CustomError(\"Patterns must be list.\")\n",
    "    \n",
    "    if d_fmt != None:\n",
    "        if d_fmt not in fmts:\n",
    "            raise CustomError(f\"Invalid d_fmt {d_fmt}.\")\n",
    "        else:\n",
    "            fmts = [d_fmt]\n",
    "\n",
    "    if d_model != None:\n",
    "        if d_model not in models:\n",
    "            raise CustomError(f\"Invalid d_model {d_model}.\")\n",
    "        else:\n",
    "            models = [d_model]\n",
    "\n",
    "    if d_size != None:\n",
    "        if d_size not in sizes:\n",
    "            raise CustomError(f\"Invalid d_size {d_size}.\")\n",
    "        else:\n",
    "            sizes = [d_size]\n",
    "\n",
    "    if d_requirements != None:\n",
    "        for d_req in d_requirements:\n",
    "            if d_req not in list(range(2, 5)):\n",
    "                raise CustomError(f\"Invalid d_requirements {d_requirements}, which {d_req} not in [2, 4].\")\n",
    "            if d_req == 4 and patterns[0] != \"clustered graph\" and len(patterns) != 0:\n",
    "                raise CustomError(\"Invalid requirement KEEP COMMUNITY STRUCTURE for non-clustered graph.\")\n",
    "        requirements = d_requirements\n",
    "\n",
    "    if d_allow_existing != None:\n",
    "        if type(d_allow_existing) != bool:\n",
    "            raise CustomError(\"d_allow_existing must be BOOL\")\n",
    "        allow_existings = [d_allow_existing]\n",
    " \n",
    "    timeout_seconds = 180\n",
    "    session_timeout = aiohttp.ClientTimeout(total=None,sock_connect=timeout_seconds,sock_read=timeout_seconds)\n",
    "    tasks = []\n",
    "    async with aiohttp.ClientSession(timeout=session_timeout)as session:\n",
    "        for p in patterns:\n",
    "            p_alias = get_p_alias(p)\n",
    "            term = \"general\" if p_alias == \"ER\" or p_alias == \"SBM\" else \"special\"\n",
    "            ids = dict()\n",
    "            ex_ids = extract_ids(f\"data/exlarge_{term}_graphs/edge_list/{p_alias}\")\n",
    "            s_ids = extract_ids(f\"data/small_{term}_graphs/edge_list/{p_alias}\")\n",
    "\n",
    "            if p_alias == \"SBM\":\n",
    "                ids[\"exlarge\"] = random.sample(ex_ids, 20)\n",
    "                ids[\"small\"] = random.sample(s_ids, 20)\n",
    "            else:\n",
    "                ids[\"exlarge\"] = ex_ids\n",
    "                ids[\"small\"] = s_ids\n",
    "\n",
    "            if d_ids != None:\n",
    "                ids[\"exlarge\"] = sorted(d_ids)\n",
    "                ids[\"small\"] = sorted(d_ids)\n",
    "    \n",
    "            test_pks = list(product(fmts, models, sizes, requirements, allow_existings))\n",
    "            if d_requirements == None and p_alias == \"SBM\":\n",
    "                test_pks.extend(list(product(fmts, models, sizes, [4], allow_existings)))\n",
    "            for test_pk in test_pks:\n",
    "                tasks.extend(await perform_test(session, test_pk, \n",
    "                                                p_alias=p_alias, ids=ids, \n",
    "                                                debug=debug, virtual=virtual, console=console))\n",
    "        if not console:\n",
    "                print(f\"Request count {len(tasks)}.\")\n",
    "        if not virtual:\n",
    "            results = await asyncio.gather(*tasks)\n",
    "\n",
    "            failed_requests = [res for res in results if \"error\" in res]\n",
    "            with open(RETRY_FILE, \"w\") as f:\n",
    "                f.write(\"\")\n",
    "            for req in failed_requests:\n",
    "                model, p_alias, fmt, size, i, q, allow_existing = req[\"PK\"] \n",
    "                with open(RETRY_FILE, \"a\") as f:\n",
    "                    f.write(f\"Faild,{q},{p_alias}_{fmt}_{size}_{i}_{allow_existing}_results.txt\\n\")\n",
    "            logging.info(f\"Failed Requests: {failed_requests}\")\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8591821f-289b-4807-848a-53acb2f4ce1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt_q2(graph_data: str, fmt: str, requirement:int, allow_existing:bool, console=False):\n",
    "\n",
    "    if type(requirement) != int:\n",
    "        raise CustomError(f\"REQUIREMENT should be int not {type(requirement)}, {requirement}\")\n",
    "    if type(allow_existing) != bool:\n",
    "        raise CustomError(f\"ALLOW_EXISTING must be BOOL {allow_existing}\")\n",
    "    \n",
    "    match requirement:\n",
    "        case 2:\n",
    "            s_requirement = \"maintain consistency between Euclidean distance and graph-theoretic distance.\"\n",
    "        case 3:\n",
    "            s_requirement = \"minimize edge crossings.\"\n",
    "        case 4:\n",
    "            s_requirement = \"keep the community structure clear. You can use module-based community detection algorithms to devide communities.\"\n",
    "        case _:\n",
    "            raise CustomError(\"Invalid requirement！\")\n",
    "\n",
    "    if not allow_existing:\n",
    "        s_allow_existing = f\"Notice, you cannot use any existing graph layout algorithms, including but not limited to nx.springlayout(), nx.circularlayout(), et cetera.\\n\"\n",
    "    else:\n",
    "        s_allow_existing = f\"Notice, you can use existing graph layout algorithms.\\n\"\n",
    "\n",
    "    ss = f\"I will provide you a graph with the format of {fmt}. Write Python code to generate a layout for the graph. The layout should {s_requirement}\\n\"\\\n",
    "    f\"{s_allow_existing}\"\\\n",
    "    f\"Do not use matplotlib or networkx to draw the graph visually after positions are determined. The final output of your code shall be the graph layout in json format. Your answer for the Python script shall be returned in a code block.\\n\"\\\n",
    "    f\"<graph data>\\n{graph_data}\"\n",
    "    content = [{\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": ss,\n",
    "        }]\n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87e2a3ad-0404-4f9b-90ca-912c520a6ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_result(rs, storage_file):\n",
    "    result_data = rs\n",
    "\n",
    "    with open(storage_file, \"w+\") as f:\n",
    "        f.write(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c372f5-73ee-48cd-91bf-ac88b1713ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def create_completion(session, content:list, p_alias:str, graph_data:str, fmt:str, model:str, size:str, i:int, req:int, allow_existing:bool, debug=False, attempt=1):\n",
    "    \n",
    "    s_allow = \"algpermitted\" if allow_existing else \"algrestricted\"\n",
    "    \n",
    "    if debug == False:\n",
    "        dir_path = f\"results_{s_allow}/{model}/{size}/{p_alias}/{req}\" # modify structure if inappropriate\n",
    "        full_res_dir_path = f\"full_reses_{s_allow}/{model}/{size}/{p_alias}/{req}\"\n",
    "    elif debug == True:\n",
    "        dir_path = f\"results_{s_allow}-debug/{model}/{size}/{p_alias}/{req}\"\n",
    "        full_res_dir_path = f\"full_reses_{s_allow}-debug/{model}/{size}/{p_alias}/{req}\"\n",
    "\n",
    "    model_url, key = get_next_endpoint_and_key()\n",
    "    headers = headers = {\n",
    "        \"Authorization\": f\"Bearer {key}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    model_alias = \"deepseek-chat\" if model == \"deepseek-v3\" else model\n",
    "\n",
    "\n",
    "    payload = {\n",
    "                \"model\": model_alias,\n",
    "                \"max_tokens\": 8000,\n",
    "                \"temperature\": 0,\n",
    "                \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\",\n",
    "                \"content\": content\n",
    "            }],\n",
    "                }\n",
    "    \n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    os.makedirs(full_res_dir_path, exist_ok=True)\n",
    "\n",
    "    \n",
    "    storage_path = f\"{dir_path}/{p_alias}_{fmt}_{size}_{i}_{req}_results.txt\"\n",
    "    full_res_path = f\"{full_res_dir_path}/{p_alias}_{fmt}_{size}_{i}_{req}_results.json\"\n",
    "    \n",
    "    async with SEMAPHORE:\n",
    "        try:\n",
    "            async with session.post(\n",
    "                url=f\"{model_url}/v1/chat/completions\",\n",
    "                json=payload,\n",
    "                headers=headers\n",
    "            ) as response:\n",
    "                if response.status == 200:\n",
    "                    no_exception = True\n",
    "                    buffer = []\n",
    "                    async for line in response.content:\n",
    "                        text = line.decode(\"utf-8\").strip()\n",
    "                        if text:\n",
    "                            buffer.append(text)\n",
    "                    full_response = \"\\n\".join(buffer)\n",
    "                    try:\n",
    "                        result = js.loads(full_response)\n",
    "                    except js.JSONDecodeError as e:\n",
    "                        no_exception = False\n",
    "                        logging.warning(f\"JSONDecodeError: {e}, {p_alias}, {model}, {size}, {i}, {req}, {s_allow}\")\n",
    "                        if attempt < MAX_RETRIES:\n",
    "                            await asyncio.sleep(RETRY_DELAY * attempt)\n",
    "                        else:\n",
    "                            logging.error(f\"Request failed after {MAX_RETRIES} attempts: {model}, {p_alias}, {size}, {i}, {s_allow}\")\n",
    "                            return {\"error\": \"None response\", \"PK\": (model, p_alias, fmt, size, i, req, allow_existing), \"attempts\": attempt}\n",
    "                    if no_exception:\n",
    "                        with open(full_res_path, \"w\") as f:\n",
    "                            js.dump(result, f)\n",
    "                        try:\n",
    "                            finish_reason = result['choices'][0]['finish_reason']\n",
    "                            if finish_reason == \"stop\":\n",
    "                                rs = result['choices'][0]['message']['content']\n",
    "                                parse_result(rs, storage_path)\n",
    "                            elif model == \"gpt-4o-2024-11-20\" and finish_reason == \"content_filter\":\n",
    "                                raise CustomError(\"Content filtered.\")\n",
    "                            else:\n",
    "                                no_exception = False\n",
    "                                print(finish_reason, f\"{model}, {p_alias}, {size}, {i}, {req}, {s_allow}\")\n",
    "                                return {\"error\": finish_reason, \"PK\": (model, p_alias, fmt, size, i, req, allow_existing), \"attempts\": attempt}\n",
    "                        except TypeError as e:\n",
    "                            no_exception = False\n",
    "                            logging.warning(f\"{e}, {model}, {p_alias}, {size}, {i}, {req}, {s_allow}\")\n",
    "                            if attempt < MAX_RETRIES:\n",
    "                                await asyncio.sleep(RETRY_DELAY * attempt)\n",
    "                            else:\n",
    "                                logging.error(f\"Request failed after {MAX_RETRIES} attempts: {model}, {p_alias}, {size}, {i}, {req}, {s_allow}\")\n",
    "                                return {\"error\": \"No content in response\", \"PK\": (model, p_alias, fmt, size, i, req, s_allow), \"attempts\": attempt}\n",
    "                        except CustomError as e:\n",
    "                            no_exception = False\n",
    "                            logging.warning(f\"Content filtered: {model}, {p_alias}, {size}, {i}, {req}, {s_allow}\")\n",
    "                            if os.path.isfile(storage_path):\n",
    "                                os.remove(storage_path)\n",
    "                            if attempt >= MAX_RETRIES:\n",
    "                                logging.error(f\"Request failed for content filtered: {model}, {p_alias}, {size}, {i}, {req}, {s_allow}\")\n",
    "                                return {\"error\": str(e), \"PK\": (model, p_alias, fmt, size, i, req, s_allow), \"attempts\": attempt}\n",
    "                            else:\n",
    "                                await asyncio.sleep(RETRY_DELAY * attempt)\n",
    "                    if no_exception:\n",
    "                        parse_result(rs, storage_path)\n",
    "                        return {\"Success\": (model, p_alias, fmt, size, i, req, s_allow)}\n",
    "                else:\n",
    "                    logging.warning(f\"Attempt  {model}, {p_alias}, {size}, {i}, {req}, {s_allow}, failed for {response.status}\")\n",
    "\n",
    "                    if attempt < MAX_RETRIES:\n",
    "                        await asyncio.sleep(RETRY_DELAY * attempt)\n",
    "                    else:\n",
    "                        logging.error(f\"Request failed after {MAX_RETRIES} attempts:  {model}, {p_alias}, {size}, {i}, {req}, {s_allow}\")\n",
    "                        return {\"error\": \"MAX_RETRY\", \"PK\": (model, p_alias, fmt, size, i, req, s_allow), \"attempts\": attempt}\n",
    "        except (aiohttp.ClientError, asyncio.TimeoutError) as e:\n",
    "            logging.warning(f\"Attempt  {model}, {p_alias}, {size}, {i}, {req}, {s_allow}, excepts for {e}\")\n",
    "\n",
    "            if attempt < MAX_RETRIES:\n",
    "                await asyncio.sleep(RETRY_DELAY * attempt)\n",
    "            else:\n",
    "                logging.error(f\"Request failed after {MAX_RETRIES} attempts:  {model}, {p_alias}, {size}, {i}, {req}, {s_allow}\")\n",
    "                return {\"error\": str(e), \"PK\": (model, p_alias, fmt, size, i, req, s_allow), \"attempts\": attempt}\n",
    "    return await create_completion(session, content, p_alias, graph_data, fmt, model, size, i, req, allow_existing, debug, attempt+1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2e2efab-3cf8-489e-a081-be2f71051e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def v_completion(content:list, model: str):\n",
    "    model_alias = \"deepseek-chat\" if model == \"deepseek-v3\" else model\n",
    "\n",
    "    json={\n",
    "        \"model\": model_alias,\n",
    "        \"max_tokens\": 8000,\n",
    "        \"temperature\": 0,\n",
    "        \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\",\n",
    "        \"content\": content\n",
    "    }],\n",
    "        }\n",
    "    return json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e18ada3-21b4-4d68-8b5d-9d58598c95c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def redo_task_by_pks(p_ks:list, debug=False, virtual=True, console=True):\n",
    "\n",
    "    timeout_seconds = 300\n",
    "    session_timeout = aiohttp.ClientTimeout(total=None,sock_connect=timeout_seconds,sock_read=timeout_seconds)\n",
    "    tasks = []\n",
    "    async with aiohttp.ClientSession(timeout=session_timeout)as session:\n",
    "        for p_k in p_ks:\n",
    "            p_alias = p_k[\"p_alias\"]\n",
    "            fmt = p_k[\"fmt\"]\n",
    "            size = p_k[\"size\"]\n",
    "            ids = dict()\n",
    "            ids[size] = [int(p_k[\"i\"])]\n",
    "            requirement = int(p_k[\"req\"])\n",
    "            term = \"general\" if p_alias == \"SBM\" else \"special\"\n",
    "            model = p_k[\"model\"]\n",
    "                    \n",
    "    \n",
    "            test_pk = (fmt, model, size, requirement)\n",
    "            tasks.extend(await perform_test(session, test_pk, p_alias, ids, debug=debug, virtual=virtual, console=console))\n",
    "        if not console:\n",
    "            print(f\"Request count {len(tasks)}.\")\n",
    "        if not virtual:\n",
    "            results = await asyncio.gather(*tasks)\n",
    "\n",
    "            failed_requests = [res for res in results if \"error\" in res]\n",
    "            logging.info(f\"Failed Requests: {failed_requests}\")\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cd6dbbf-f93a-4254-81c9-8767a0f3bbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def redo_task_from_file(model:str, filename: str, virtual=True, console=False):\n",
    "    with open(filename, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        p_ks = []\n",
    "        for l in lines:\n",
    "            no_pair_specified = None\n",
    "            _, q, data_p_k = l.split(\",\")\n",
    "            p_alias, fmt_1st_half, _, size, i, __ = data_p_k.split(\"_\")\n",
    "            fmt = f\"{fmt_1st_half}_list\"\n",
    "            d = {\n",
    "                \"model\": model,\n",
    "                \"p_alias\": p_alias,\n",
    "                \"fmt\": fmt,\n",
    "                \"size\": size,\n",
    "                \"i\": i,\n",
    "                \"req\": q\n",
    "            }\n",
    "            p_ks.append(d)\n",
    "        await redo_task_by_pks(p_ks, virtual=virtual, console=console)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349bda52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example usage:\n",
    "patterns = [\"clustered graph\"]\n",
    "await do_task(patterns, d_size=\"small\", debug=False, virtual=False, console=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
