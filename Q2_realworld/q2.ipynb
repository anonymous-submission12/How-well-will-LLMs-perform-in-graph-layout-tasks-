{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2ff773-ea3c-44f1-af96-5f2088831324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import os\n",
    "import json as js\n",
    "from itertools import product\n",
    "from itertools import cycle\n",
    "import time\n",
    "from pathlib import Path\n",
    "import networkx as nx\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aade1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomError(RuntimeError):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292c5586-f485-48e0-961f-3a7f8523de2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = Path(\"logs/async_requests.log\")\n",
    "log_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file, mode=\"a\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa6cf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 3 #DELAY BY SECONDS\n",
    "\n",
    "RETRY_FILE = \"retry_gpt.txt\"\n",
    "\n",
    "SEMAPHORE = asyncio.Semaphore(50) # Decrease if toooo big\n",
    "\n",
    "endpoints = \"YOUR END POINTS\"\n",
    "api_keys = \"YOUR API KEYS\"\n",
    "endpoint_cycle = cycle(endpoints)\n",
    "api_key_cycle = cycle(api_keys)\n",
    "\n",
    "def get_next_endpoint_and_key():\n",
    "    raise NotImplementedError()\n",
    "    #return next(endpoint_cycle), next(api_key_cycle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8056fa75-c08a-4f57-b1a1-3acf526d6328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_graph_data_by_init(init:str) -> str:\n",
    "    \"\"\"Find the path of graph data file for INIT\"\"\"\n",
    "    match init:\n",
    "        case \"dolphins\":\n",
    "            return \"data/dolphins_transformed.json\"\n",
    "        case \"karate\":\n",
    "            return \"data/Karate club(34 nodes).json\"\n",
    "        case \"lesmis\":\n",
    "            return \"data/lesmis.json\"\n",
    "        case _:\n",
    "            raise CustomError(f\"Invalid INIT {init}. Init shall be one of \\\"dolphins\\\", \\\"karate\\\" or \\\"lesmis\\\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9522174c-1f97-4147-8e77-5d82f60792d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_graph_from_edge_list(filename:str):\n",
    "    with open(filename, \"r\") as f:\n",
    "        graph = js.load(f)\n",
    "    G = nx.Graph(graph[\"edges\"])\n",
    "    \n",
    "    return sorted(G.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293c2d3d-5cb9-461e-9458-a43e27faf56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_graph_to_str_el(f):\n",
    "    e = read_graph_from_edge_list(f)\n",
    "    return str(e)\n",
    "\n",
    "async def perform_test(session, test_pks:list, debug=False, virtual=True, console=True):\n",
    "    tasks = []\n",
    "    valid_requirements = list(range(1, 5))\n",
    "    \n",
    "    init, model, requirement, allow_existing = test_pks\n",
    "\n",
    "    if requirement not in valid_requirements:\n",
    "        raise CustomError(f\"Invalid requirement {requirement}.\")\n",
    "\n",
    "\n",
    "    f = find_graph_data_by_init(init)\n",
    "    graph_data = parse_graph_to_str_el(f)\n",
    "\n",
    "        \n",
    "    content = generate_prompt_q2(graph_data, requirement, allow_existing, console=virtual)\n",
    "    if not virtual: \n",
    "        tasks.append(create_completion(session, content, init, graph_data, model, requirement, allow_existing, debug=debug))\n",
    "    else:\n",
    "        v_c = v_completion(content, model)\n",
    "        tasks.append(v_c)\n",
    "        if console:\n",
    "            print(v_c)\n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fd280a-57b4-43c5-a4fa-5255c96a8a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def do_task(d_inits:list=None, d_model=None, d_requirements:list=None, d_allow_existing:bool=None, debug=False, virtual=True, console=True):\n",
    "    \"\"\"Perform tests on task data.\\n\n",
    "        Parameters starting with \"d_\" are optional, use them if dimensions of the task are specified.\\n\n",
    "        Set DEBUG to True to test with a few runs.\\n\n",
    "        Set VIRTUAL to True to inspect the prompts generated without sending any actual requests.\n",
    "        Set CONSOLE to False to disable print statements.\n",
    "        \"\"\"\n",
    "    inits = [\"dolphins\", \"karate\", \"lesmis\"]\n",
    "    models = [\"gpt-4o-2024-11-20\", \"gemini-2.0-flash-001\", \"deepseek-v3\"]\n",
    "    requirements = list(range(2, 5))\n",
    "    allow_existings = [True, False]\n",
    "    \n",
    "    if d_inits != None:\n",
    "        for d_init in inits:\n",
    "            if d_init not in d_init:\n",
    "                raise CustomError(f\"Invalid d_init {d_init}.\")\n",
    "        inits = d_inits\n",
    "\n",
    "    if d_model != None:\n",
    "        if d_model not in models:\n",
    "            raise CustomError(f\"Invalid d_model {d_model}.\")\n",
    "        else:\n",
    "            models = [d_model]\n",
    "\n",
    "    if d_requirements != None:\n",
    "        for d_req in d_requirements:\n",
    "            if d_req not in list(range(2, 5)):\n",
    "                raise CustomError(f\"Invalid d_requirements {d_requirements}, which {d_req} not in [2, 4].\")\n",
    "        requirements = d_requirements\n",
    "\n",
    "    if d_allow_existing != None:\n",
    "        if type(d_allow_existing) != bool:\n",
    "            raise CustomError(\"d_allow_existing must be BOOL\")\n",
    "        allow_existings = [d_allow_existing]\n",
    " \n",
    "    timeout_seconds = 180\n",
    "    session_timeout = aiohttp.ClientTimeout(total=None,sock_connect=timeout_seconds,sock_read=timeout_seconds)\n",
    "    tasks = []\n",
    "    async with aiohttp.ClientSession(timeout=session_timeout)as session:\n",
    "        test_pks = list(product(inits, models, requirements, allow_existings))\n",
    "        for test_pk in test_pks:\n",
    "            tasks.extend(await perform_test(session, test_pk, debug=debug, virtual=virtual, console=console))\n",
    "        if not console:\n",
    "                print(f\"Request count {len(tasks)}.\")\n",
    "        if not virtual:\n",
    "            results = await asyncio.gather(*tasks)\n",
    "\n",
    "            failed_requests = [res for res in results if \"error\" in res]\n",
    "            with open(RETRY_FILE, \"w\") as f:\n",
    "                f.write(\"\")\n",
    "            for req in failed_requests:\n",
    "                init, model, q, allow_existing = req[\"PK\"] \n",
    "                with open(RETRY_FILE, \"a\") as f:\n",
    "                    f.write(f\"Faild,{init}_{model}_{q}_{allow_existing}_results.txt\\n\")\n",
    "            logging.info(f\"Failed Requests: {failed_requests}\")\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8591821f-289b-4807-848a-53acb2f4ce1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt_q2(graph_data: str, requirement:int, allow_existing:bool, console=False):\n",
    "\n",
    "    if type(requirement) != int:\n",
    "        raise CustomError(f\"REQUIREMENT should be int not {type(requirement)}, {requirement}\")\n",
    "    if type(allow_existing) != bool:\n",
    "        raise CustomError(f\"ALLOW_EXISTING must be BOOL {allow_existing}\")\n",
    "    \n",
    "    match requirement:\n",
    "        case 2:\n",
    "            s_requirement = \"maintain consistency between Euclidean distance and graph-theoretic distance.\"\n",
    "        case 3:\n",
    "            s_requirement = \"minimize edge crossings.\"\n",
    "        case 4:\n",
    "            s_requirement = \"keep the community structure clear. You can use module-based community detection algorithms to devide communities.\"\n",
    "\n",
    "    if not allow_existing:\n",
    "        s_allow_existing = f\"Notice, you cannot use any existing graph layout algorithms, including but not limited to nx.springlayout(), nx.circularlayout(), et cetera.\\n\"\n",
    "    else:\n",
    "        s_allow_existing = f\"Notice, you can use existing graph layout algorithms.\\n\"\n",
    "\n",
    "    ss = f\"I will provide you a graph with the format of edge_list. Write Python code to generate a layout for the graph. The layout should {s_requirement}\\n\"\\\n",
    "    f\"{s_allow_existing}\"\\\n",
    "    f\"Do not use matplotlib or networkx to draw the graph visually after positions are determined. The final output of your code shall be the graph layout in json format. Your answer for the Python script shall be returned in a code block.\\n\"\\\n",
    "    f\"<graph data>\\n{graph_data}\"\n",
    "    content = [{\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": ss,\n",
    "        }]\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87e2a3ad-0404-4f9b-90ca-912c520a6ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_result(rs, storage_file):\n",
    "    result_data = rs\n",
    "\n",
    "    with open(storage_file, \"w+\") as f:\n",
    "        f.write(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c372f5-73ee-48cd-91bf-ac88b1713ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def create_completion(session, content:list, init:str, graph_data:str, model:str, req:int, allow_existing:bool, debug=False, attempt=1, official=False):\n",
    "    \n",
    "    s_allow = \"algpermitted\" if allow_existing else \"algrestricted\"\n",
    "    \n",
    "    if debug == False:\n",
    "        dir_path = f\"results/{s_allow}/{model}/{req}\" # modify structure if inappropriate\n",
    "        full_res_dir_path = f\"full_reses/{s_allow}/{model}/{req}\"\n",
    "    elif debug == True:\n",
    "        dir_path = f\"results-debug/{s_allow}/{model}/{req}\"\n",
    "        full_res_dir_path = f\"full_reses-debug/{s_allow}/{model}/{req}\"\n",
    "\n",
    "    model_url, key = get_next_endpoint_and_key()\n",
    "    headers = headers = {\n",
    "        \"Authorization\": f\"Bearer {key}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    model_alias = \"deepseek-chat\" if model == \"deepseek-v3\" else model\n",
    "\n",
    "\n",
    "    payload = {\n",
    "                \"model\": model_alias,\n",
    "                \"max_tokens\": 8000,\n",
    "                \"temperature\": 0,\n",
    "                \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\",\n",
    "                \"content\": content\n",
    "            }],\n",
    "                }\n",
    "    \n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    os.makedirs(full_res_dir_path, exist_ok=True)\n",
    "\n",
    "    \n",
    "    storage_path = f\"{dir_path}/{init}_{req}_results.txt\"\n",
    "    full_res_path = f\"{full_res_dir_path}/{init}_{req}_results.json\"\n",
    "    \n",
    "    async with SEMAPHORE:\n",
    "        try:\n",
    "            async with session.post(\n",
    "                url=f\"{model_url}/v1/chat/completions\",\n",
    "                json=payload,\n",
    "                headers=headers\n",
    "            ) as response:\n",
    "                if response.status == 200:\n",
    "                    no_exception = True\n",
    "                    buffer = []\n",
    "                    async for line in response.content:\n",
    "                        text = line.decode(\"utf-8\").strip()\n",
    "                        if text:\n",
    "                            buffer.append(text)\n",
    "                    full_response = \"\\n\".join(buffer)\n",
    "                    try:\n",
    "                        result = js.loads(full_response)\n",
    "                    except js.JSONDecodeError as e:\n",
    "                        no_exception = False\n",
    "                        logging.warning(f\"JSONDecodeError: {e}, {init}, {model}, {req}, {s_allow}\")\n",
    "                        if attempt < MAX_RETRIES:\n",
    "                            await asyncio.sleep(RETRY_DELAY * attempt)\n",
    "                        else:\n",
    "                            logging.error(f\"Request failed after {MAX_RETRIES} attempts: {init}, {model}, {req}, {s_allow}\")\n",
    "                            return {\"error\": \"None response\", \"PK\": (init, model, req, allow_existing), \"attempts\": attempt}\n",
    "                    if no_exception:\n",
    "                        with open(full_res_path, \"w\") as f:\n",
    "                            js.dump(result, f)\n",
    "                        try:\n",
    "                            finish_reason = result['choices'][0]['finish_reason']\n",
    "                            if finish_reason == \"stop\":\n",
    "                                rs = result['choices'][0]['message']['content']\n",
    "                                parse_result(rs, storage_path)\n",
    "                            elif model == \"gpt-4o-2024-11-20\" and finish_reason == \"content_filter\":\n",
    "                                raise CustomError(\"Content filtered.\")\n",
    "                            else:\n",
    "                                no_exception = False\n",
    "                                print(finish_reason, f\"{init}, {model}, {req}, {s_allow}\")\n",
    "                                return {\"error\": finish_reason, \"PK\": (init, model, req, allow_existing), \"attempts\": attempt}\n",
    "                        except TypeError as e:\n",
    "                            no_exception = False\n",
    "                            logging.warning(f\"{e}, {init}, {model}, {req}, {s_allow}\")\n",
    "                            if attempt < MAX_RETRIES:\n",
    "                                await asyncio.sleep(RETRY_DELAY * attempt)\n",
    "                            else:\n",
    "                                logging.error(f\"Request failed after {MAX_RETRIES} attempts: {init}, {model}, {req}, {s_allow}\")\n",
    "                                return {\"error\": \"No content in response\", \"PK\": (init, model, req, allow_existing), \"attempts\": attempt}\n",
    "                        except CustomError as e:\n",
    "                            no_exception = False\n",
    "                            logging.warning(f\"Content filtered: {init}, {model}, {req}, {s_allow}\")\n",
    "                            if os.path.isfile(storage_path):\n",
    "                                os.remove(storage_path)\n",
    "                            if attempt >= MAX_RETRIES:\n",
    "                                logging.error(f\"Request failed for content filtered: {init}, {model}, {req}, {s_allow}\")\n",
    "                                return {\"error\": str(e), \"PK\": (init, model, req, allow_existing), \"attempts\": attempt}\n",
    "                            else:\n",
    "                                await asyncio.sleep(RETRY_DELAY * attempt)\n",
    "                    if no_exception:\n",
    "                        parse_result(rs, storage_path)\n",
    "                        return {\"Success\": (init, model, req, allow_existing)}\n",
    "                else:\n",
    "                    logging.warning(f\"Attempt {init}, {model}, {req}, {s_allow}, failed for {response.status}\")\n",
    "\n",
    "                    if attempt < MAX_RETRIES:\n",
    "                        await asyncio.sleep(RETRY_DELAY * attempt)\n",
    "                    else:\n",
    "                        logging.error(f\"Request failed after {MAX_RETRIES} attempts: {init}, {model}, {req}, {s_allow}\")\n",
    "                        return {\"error\": \"MAX_RETRY\", \"PK\": (init, model, req, allow_existing), \"attempts\": attempt}\n",
    "        except (aiohttp.ClientError, asyncio.TimeoutError) as e:\n",
    "            logging.warning(f\"Attempt {init}, {model}, {req}, {s_allow}, excepts for {e}\")\n",
    "\n",
    "            if attempt < MAX_RETRIES:\n",
    "                await asyncio.sleep(RETRY_DELAY * attempt)\n",
    "            else:\n",
    "                logging.error(f\"Request failed after {MAX_RETRIES} attempts: {init}, {model}, {req}, {s_allow}\")\n",
    "                return {\"error\": str(e), \"PK\": (init, model, req, allow_existing), \"attempts\": attempt}\n",
    "    return await create_completion(session, content, init, graph_data, model, req, allow_existing, debug, attempt+1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2e2efab-3cf8-489e-a081-be2f71051e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def v_completion(content:list, model: str):\n",
    "    model_alias = \"deepseek-chat\" if model == \"deepseek-v3\" else model\n",
    "\n",
    "    json={\n",
    "        \"model\": model_alias,\n",
    "        \"max_tokens\": 8000,\n",
    "        \"temperature\": 0,\n",
    "        \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\",\n",
    "        \"content\": content\n",
    "    }],\n",
    "        }\n",
    "    return json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f45a04-9e3a-46ac-8662-b112ed0018e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example usage:\n",
    "await do_task(d_model=\"gpt-4o-2024-11-20\", debug=False, virtual=False, console=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
